---
title: "Week of June 14th"
date: '2020-06-14'
categories:
  - weekly
tags:
  - neural nets
  - PyTorch
  - Image Classification
---

## What I did

- I figured out what was wrong with Colab's performance off of GPU.
- I did a lab report on EPMA, which involved writing a python script to automate a bunch of conversions from weight percent to atomic percent.

## What I learned

- Turns out trying to read data from a mounted Google Drive cripples it. Additionally, when using a model with pytroch lightning, you don't need to due a ```model.to(device)```. It takes care of that for you. Doing it myself was screwing things over. You need to download the dataset onto the notebook by something like this:

```python
trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=my_transform)

trainloader = torch.utils.data.DataLoader(trainset,shuffle=True, num_workers=2)
```

or better yet download the proper dataset as follows:

```python
!mkdir /datasets
!mkdir /datasets/cifar10
!cd /datasets/cifar10 && wget https://cdn3.vision.in.tum.de/~dl4cv/cifar10.zip --no-check-certificate
!cd /datasets/cifar10 && unzip -q cifar10.zip
```

- It's best to put dropout layers after a ReLU activation and before an affine (nn.Linear()) layer
- You can achieve high scores on CIFAR 10 without convolution if you use bottling layers.

### Here the process for building a NN for cifar10

You need to start with a simple model first. Pick some hype
hyperparameters to see if you can overfit it (say, make the training set 1 - 10 images). If you can overfit it, then it means your code is working and your architecture makes sense. Next you can up the images to say 1000 and run a broad hyperparameter search. Afterwords, run maybe 20 epochs and see how the training and validation loss are moving. If they both are going down, your architecture looks good. If not, start over.

Once it looks like your training loss is continuing to decrease but your validation loss plateaus, it's time to start introducing regularization (say nn.Dropout(.5)) and data augmentation. This way you won't continue to overfit the data and can keep dropping the validation loss.

I haven't done it yet with the cifar10 set, but I suspect that you should build up a more complex architecture without regularization and make sure you can overfit it to death before tuning hyperparameters and introducing regularization.

>*If you start with a complex model with data augmentation and regularization you'll never be able to tune the hyperparameters and find a good solution.*

## What I will do next

I need to finish that Code Academy course on chatbots ASAP before my free pro account expires.

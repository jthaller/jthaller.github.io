---
title: "Week of May 17th"
excerpt_separator: "<!--more-->"
categories:
  - weekly
tags:
  - Neural Net
  - Coursework
---

# What I did
- Deep Learning coursework. This week's was difficult, mostly because they segmented the code in a way that was pretty unclear. For example, backpropagation was split into three different functions in three different classes. It was frustrating.
- Read another few chapters of my stats textbook. These were mostly a review of hypothesis testing and associated terms. 
- I thought about how I would build a word cloud for my facebook chat data.

# What I learned
- I realized that I didn't understand backpropagation as well as I thought. See *what I will do next*.
- Here are some important definitions to remember:

```
*null hypothesis*:
*One- vs. two-tailed tests*:
*significance factor*
*test power*:
```

# What I will do next
- I think I would benefit greatly from writing an article about how it works. It would likely have to be quite lengthy though, so I might create a separate post for it. It's also quite difficult to explain without figures, so it would take me some time to do it well.

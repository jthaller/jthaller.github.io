---
title: "Week of May 17"
categories:
  - weekly
tags:
  - Neural Net
  - Coursework
---

# What I did
- Deep Learning coursework. This week's was difficult, mostly because they segmented the code in a way that was pretty unclear. For example, backpropagation was split into three different functions in three different classes. It was frustrating.
- Read another few chapters of my stats textbook. These were mostly a review of hypothesis testing and associated terms. 
- I thought about how I would build a word cloud for my facebook chat data.

# What I learned
- I realized that I didn't understand backpropagation as well as I thought. See *what I will do next*.
- Here are some important definitions to remember:


>*null hypothesis*: The originatoriginal hypothesis you're testing.
>*One- vs. two-tailed tests*: If null hypothesis has â‰  vs. > or <
>*significance factor*: Alpha, the area of the normal curve outside your confidence interval
>*test power*: On the internal of [0,1], 0 means your H-test is worthless, 1 means it's really good.  


# What I will do next
- I think I would benefit greatly from writing an article about how it works. It would likely have to be quite lengthy though, so I might create a separate post for it. It's also quite difficult to explain without figures, so it would take me some time to do it well.

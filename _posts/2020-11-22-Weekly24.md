---
title: "Week of November 22nd"
date: '2020-11-22'
categories:
  - weekly
tags:
---

## What I did

- I finished the bayesian stats course on Saturday, but I wanted to redo the final exercises in python instead of R.

## What I learned

- So, I already learned about the p-value test statistic, but I hadn't seen it in the context of a linear regression. It struck me as odd and I needed to look up what to what it actually was referring.

```python
from scipy import stats
slope, intercept, r_value, p_value, std_err = stats.linregress(x,y)
```

According to the documentation, the p_value returns a "Two-sided p-value for a hypothesis test whose null hypothesis is that the slope is zero, using Wald Test with t-distribution of the test statistic." Essentially, what is the probability of obtaining a result at least as extreme as the measured slope, assuming the null hypothesis of zero. For a 2-tailed test, the p-value is:

$$
p-value = 2P(Z > |z_0|) \\
\text{where} Z_0 = \frac{\bar{x} - \mu_0}{\sigma/\sqrt{n}}
$$

## What I Will do next


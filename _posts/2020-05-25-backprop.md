---
title: "The dance of Forward-Pass and Backprop"
date: '2020-05-25'
categories:
  - miscellaneous
tags:
  - Neural Net
---

( *in progress* ... )

In order to impliment a neural network, a solid understanding of Linear Algebra is needed for the forward pass, whereas vector calculus is needed for backpropagation. Neither action is particularly complicated, mathetmatically, howerever there are so many moving parts it is easy to get lost in the sea of similar sounding partial derivatives. Here, for my benefit, I'm going to walk through the math, as well as the full process of implimenting the two function for an affine neural network with 2 hidden layers.

## Feed Forward and Einstein Notation

There are enough captial sigmas in the world. I haven't seen anyone use Einstein notation to represent the forward passes of a neural network. Perhaps this is because in practice one usually uses an activation function between each layer.

For a one layer (affine) neural network, the forward propogation is calculated as follows using Einstein notation:

![alt]({{ site.url }}{{ site.baseurl }}/assets/images/einstein_NN.png)
{: .full}

Recall that repeated indices are implicitely summed over. Explicitely, for one component of the hidden layer:

$$
\being{align}
h_j & = \text{sigmoid}(w_{1j}x_1 + w_{2j}x_2 + w_{3j}x_3 + w_{4j}x_4 + w_{5j}x_5 + b_j) \\
& = \text{sigmoid}\Big(\sum_{i=1}^{i=5}w_{ij}x_{i} + b_j\Big)]
\end{align}
$$

And in vectorized form:

![alt]({{ site.url }}{{ site.baseurl }}/assets/images/NN_forward.png)
{: .full}

The process can be repeated ad-nauseum for networks with more layers.

## Backpropagation
First, I will show the partial derivative chain rules, and then do it again, more explicitely with defined functions. For now, say your functional is:

$$\hat{y} = \sigma(g(f(x,y)))$$.

This would be a network with inputs (x,y), two hidden layer (f and g), and an output with activation function sigma. Lets say the loss is the RMS loss. Thus,

$$
L = 1/m \Sigma (\hat{y} - y)^2
$$

To see how much we need to shift the weights, we need to find the gradients for each layer. For the first partial derivitive, we have:

$$
\frac{\partial L}{\partial L} = 1
$$

Trivial. Time for the next layer.

$$
\frac{\partial L}{\partial \sigma} = \frac{\partial L}{\partial \hat{y}} \frac{\partial \hat{y}}{\partial \sigma}
$$

Next layer:

$$
\frac{\partial L}{\partial g} = \frac{\partial L}{\partial \hat{y}} \frac{\partial \hat{y}}{\partial \sigma} \frac{\partial \sigma}{\partial g}
$$

---
title: "The dance of Forward-Pass and Backprop"
date: '2020-05-25'
categories:
  - miscellaneous
tags:
  - Neural Net
---

( *in progress* ... )

In order to impliment a neural network, a solid understanding of Linear Algebra is needed for the forward pass, whereas vector calculus is needed for backpropagation. Neither action is particularly complicated, mathetmatically, howerever there are so many moving parts it is easy to get lost in the sea of similar sounding partial derivatives. Here, for my benefit, I'm going to walk through the math, as well as the full process of implimenting the two function for an affine neural network with 2 hidden layers.

# Feed Forward and Einstein Notation
There are enough captial sigmas in the world. I haven't seen anyone use Einstein notation to represent the forward passes of a neural network. Perhaps this is because in practice one usually uses an activation function between each layer.

For a one layer (affine) neural network, the forward propogation is calculated as follows using Einstein notation:

![alt]({{ site.url }}{{ site.baseurl }}/assets/images/einstein_NN.png)
{: .full}

Explicitely, in vector form:

![a_j= \text{sigmoid}(w_{1j}x_1 + w_{2j}x_2 + w_{3j}x_3 + w_{4j}x_4 + w_{5j}x_5+ b_j) = \text{sigmoid}\Big(\sum_{i=1}^{i=5}w_{ij}x_{i} + b_j\Big)](https://render.githubusercontent.com/render/math?math=a_j%3D%20%5Ctext%7Bsigmoid%7D(w_%7B1j%7Dx_1%20%2B%20w_%7B2j%7Dx_2%20%2B%20w_%7B3j%7Dx_3%20%2B%20w_%7B4j%7Dx_4%20%2B%20w_%7B5j%7Dx_5%2B%20b_j)%20%3D%20%5Ctext%7Bsigmoid%7D%5CBig(%5Csum_%7Bi%3D1%7D%5E%7Bi%3D5%7Dw_%7Bij%7Dx_%7Bi%7D%20%2B%20b_j%5CBig))

![alt]({{ site.url }}{{ site.baseurl }}/assets/images/NN_forward.png)
{: .full}



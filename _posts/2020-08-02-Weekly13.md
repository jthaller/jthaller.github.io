---
title: "Week of August 2nd"
date: '2020-08-22'
categories:
  - weekly
tags:
  - CNN
  - NN
---

## What I did

- Made a word cloud from my facebook messenger data. I realized that this was on my resume, but I never actually did it.
It only took a day, since I already had the data and a solid understanding of its organization.

## What I learned

- There's some known bug in the facebook encoding scheme of the JSON file. This could be a reason why my chatbot was spitting
out nonsense, though I suspect there are more issues with the chatbot project that I need to address. From what I found on the web,
the JSON is encoded in `uft-8`, but gets decoded as `latin-1`, before being loaded as `utf-8`. To remedy this, reversed the process; I loaded the JSON, then went through line by line, encoding it in `latin-1`, then decoding it as `utf-8` again.

```python
def parse_obj(obj):
    for key in obj:
        if isinstance(obj[key], str):
            obj[key] = obj[key].encode('latin_1').decode('utf-8')
        elif isinstance(obj[key], list):
            obj[key] = list(map(lambda x: x if type(x) != str else x.encode('latin_1').decode('utf-8'), obj[key]))
        pass
    return obj

with open(sarah_json1) as f:
     fixed_json = json.load(f, object_hook=parse_obj)
     df = pd.json_normalize(fixed_json["messages"])
```

- I'm still facing issues with apostrophes. These don't seem to be decoded correctly. I still have `u'\u2019'` (i.e. right single quotation
marks) that show up. I can't seem regex-replace them. This is particularly troublesome for this project because the word-cloud package
interprets that symbol as cause to split the word in too. Thus, it'll split a word like `'they're` into `they` and `re`. The `STOPWORDS` package removes `they` from the cloud, but obviously `re` isn't a word, but it is super common since `they're` is a common word. For now I've just added all the phonemes that follow apostrophes to the list of stop-words, but I need to fix this issue for the chatbot training data.

## What I will do next

- It might be more interesting to make word nets for each part of speech. I found some code I wrote earlier that uses word net to split the message corpus into parts of speech.

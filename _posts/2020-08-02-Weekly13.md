---
title: "Week of August 2nd"
date: '2020-08-22'
categories:
  - weekly
tags:
  - CNN
  - NN
---

## What I did

- Made a word cloud from my facebook messenger data. I realized that this was on my resume, but I never actually did it.
It only took a day, since I already had the data and a solid understanding of its organization.

## What I learned

- There's some known bug in the facebook encoding scheme of the JSON file. This could be a reason why my chatbot was spitting
out nonsense, though I suspect there are more issues with the chatbot project that I need to address. From what I found on the web,
the JSON is encoded in `uft-8`, but gets decoded as `latin-1`, before being loaded as `utf-8`. To remedy this, reversed the process; I loaded the JSON, then went through line by line, encoding it in `latin-1`, then decoding it as `utf-8` again.

```python
def parse_obj(obj):
    for key in obj:
        if isinstance(obj[key], str):
            obj[key] = obj[key].encode('latin_1').decode('utf-8')
        elif isinstance(obj[key], list):
            obj[key] = list(map(lambda x: x if type(x) != str else x.encode('latin_1').decode('utf-8'), obj[key]))
        pass
    return obj

with open(sarah_json1) as f:
     fixed_json = json.load(f, object_hook=parse_obj)
     df = pd.json_normalize(fixed_json["messages"])
```

- I'm still facing issues with apostrophes. These don't seem to be decoded correctly. I still have `u'\u2019'` (i.e. right single quotation
marks) that show up. I can't seem regex-replace them. This is particularly troublesome for this project because the word-cloud package
interprets that symbol as cause to split the word in too. Thus, it'll split a word like `'they're` into `they` and `re`. The `STOPWORDS` package removes `they` from the cloud, but obviously `re` isn't a word, but it is super common since `they're` is a common word. For now I've just added all the phonemes that follow apostrophes to the list of stop-words, but I need to fix this issue for the chatbot training data.

### L1 vs L2 regularization

- L2 regularization is often referred to as weight decay, because:

$$
W = (1-\alpha \lambda)W - \frac{\partial J}{\partial W}
$$

where $\alpha$ is the learning rate, $\lambda$ is the regularization hyper-parameter, and J is the cost. Every iteration the weights are pushed closer to zero since your multiplying the weights by a number $<1$. For L2, we have:

$$
cost = \Sigma (y_i - y)^2 + \lambda * \Sigma(W)^2
$$

- L1 regularization is just the same as above, but with an absolute value for the regularization term instead of a square. L1 is known as LASSO (least absolute shrinkage and selection operator), because it shrinks the less important features' coefficients to zero. This is because for small values $|w|$ is a much stiffer penalty than $|w^2|$. Thus, L1 is thus a good choice when you have a ton of features.

### Famous Network Architectures

out = [in - kernal_size + 2*(padding)]/stride +1

- AlexNet
  - CNN first used with huge success in predicting IMAGE-NET (1000 classes).
  - 5 convolutional layers, 3 fully-connected layers, ReLU activations
![Alex](https://miro.medium.com/max/875/1*bD_DMBtKwveuzIkQTwjKQQ.png)




## What I will do next

- It might be more interesting to make word nets for each part of speech. I found some code I wrote earlier that uses word net to split the message corpus into parts of speech.

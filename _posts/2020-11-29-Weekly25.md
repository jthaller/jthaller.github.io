---
title: "Week of November 29th"
date: '2020-11-29'
categories:
  - weekly
tags:
---

## What I did


## What I learned

- **Homoskedasticity**
    - Interested in the behavior of our errors/residuals. If the variance of our errors given independent variable x is constant, we have homoskedasticity.

    > $$var(a_i \vert x_i) = constant$$

- **Heteroskedasticity**

    - When the errors are not constant, and the size of the errors is a function of your independent variable.

    > $$var(a_i \vert x_i) = f(x_i)$$

    - Commonly, this might mean that your model gets worse at predicting as the x values increase. This could indicate a few things. You might not be taking into account enough/all the information needed for OLS to work well. Another least squares algorithm could perform better (be homoskedastic). You could also try a different type of regression or a more complicated ML model.

![](https://i.ytimg.com/vi/zRklTsY9w9c/maxresdefault.jpg)

- Data leakage is when information from outside the training dataset is used to create the model. [good quiz question](https://www.confetti.ai/questions/109-0)

## What I will do next


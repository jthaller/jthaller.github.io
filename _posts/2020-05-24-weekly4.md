---
title: "Week of May 24"
date: '2020-05-24'
categories:
  - weekly
tags:
  - Neural Net
  - Coursework
---

# What I did

- Reviewed Einstein notation. I decided if I really want to understand neural networks, and not just know how they work, I need to drill the math into my head. There are a lot of inner products and summations, so it seemed fitting to write everything in Einstein notation.
- I've been thinking about starting a new kaggle project and moving on from NLP slightly. The jobs I'm most interested in, at the moment, are ones where implimenting recommender system is the main objective i.e. suggesting movies on netflix, songs on spotify, or art up for auction at Sotheby's.


# What I learned

### **Jaccard Idex**

A statistic used to represent the similarity (or diversity) of sample sets. For two sets, A and B, the Jaccard index, J, is calculated as follows.

```matlab
A = {0,1,2,5,6}
B = {0,2,3,4,5,7,9}
Solution:
J(A,B) = |A∩B| / |A∪B| = |{0,2,5}| / |{0,1,2,3,4,5,6,7,9}| = 3/9 = 0.33.
```

### **Jaccard Distance**

A similar statistic to the above, but this measures the *dissimilarity* between two sets. The two must add to one.

```matlab
D(A,B) = 1 – J(A,B)
```

## Adaptive Learning Rate Optimization Algorithms

### **Stochastic Gradient Descent with Momentum**

 Compared to regular SGD, this can greatly reduce time to convergence. The general idea is add a fraction of the previous update to the current update.  The *exponetional moving average* is a averaging of points within a period that puts greater weight on more recent points (a simple moving average treats each points as equally significant). Here, V is the EMA. Note that the numbers should be subscripts.:

```python
V(1)= βV[0] + (1 — β)S[1]
V(2)= βV[1] + (1 — β)S[2]
*etc.*
```
S is the sequence. EMA's are common in market forcasting, so often S is price at time t.
β is defined from [0,1], and usually a value around 0.90 is used. The continuous update for SGD with momentum, using Andrew Ng's notation, is as follows:

```python
V[1] := β *V[0]+ (1 — β) ∇_w L(W, X, y)
W := W - αV[1]
```

α is the learning rate, as always. To be clear, ∇_w L is the gradient of the loss function with respect to the weights. \
SGD with momentum tends to work better than SGD because it gives a closer estimate of the full gradient from the batch than SGD. Additionally, the momentum helps push the update through a ravine in the right direction, where SGD tends to oscillate back and forth on in the short way. In my course's notation:

```python
v[k+1] = β v[k] + ∇_θ L(θ[k])
α[k+1] = θ[k] - α * v[k+1]
```

Here, v is the velocity term, beta is the adjustable momentum term, θ is the model parameters (i.e. the weights or biases), and α i the learning rate as always.

### **RMS Prop**

RMS prop is another varient of SGD to improve convergence speed. Once again, the idea is to dampen oscillations in directions where you are close to the minimum, and accelerate movement when you are far from the minimum. Here, we keep a moving average of the squared gradients for each weight. As before, ∇_θ L is the gradient of the cost with respect to weights.

```python
s[k+1] =  β S[k] + (1 - β)( ∇_θ L • ∇_θ L )
θ[k+1] = θ[k] - α (∇_θ L)/( Sqrt(S[k+1]) + Ɛ )
```

 Per physics convention, Ɛ is a tiny value. It is added so that if the Sqrt term gets too small the value doesn't blow up.

### **Adam Optimizer**

Adam is a combination of RMS prop and SGD with momentum.

### **Math Review:**

The gradient acts on a scalar field (scalar valued differentiable function) and returns a vector. For example, ∇v = [dv/dx, dv/dy, dv/dz]. The gradient points in the direction of greatest increase. \
The divergence acts on a vector field and returns a scalar. It is the outward flux of a vector field around a single point. For example, ∇•v = dv/dx + dv/y + dv/dz.

# What I will do next

- I really need to figure out a better way of doing Latex in Jekyll. The ones I tried today were imcompatable with my theme.

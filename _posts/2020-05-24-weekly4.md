---
title: "Week of May 24"
date: '2020-05-24'
categories:
  - weekly
tags:
  - Neural Net
  - Coursework
---

( *In progress...* )

# What I did

- Reviewed Einstein notation. I decided if I really want to understand neural networks, and not just know how they work, I need to drill the math into my head. There are a lot of inner products and summations, so it seemed fitting to write everything in Einstein notation.
- I've been thinking about starting a new kaggle project and moving on from NLP slightly. The jobs I'm most interested in, at the moment, are ones where implimenting recommender system is the main objective i.e. suggesting movies on netflix, songs on spotify, or art up for auction at Sotheby's.

# What I learned

### **Jaccard Validation**

### **Stochastic Gradient Descent with Momentum**

 Compared to regular SGD, this can greatly reduce time to convergence. The general idea is add a fraction of the previous update to the current update.  The *exponetional moving average* is a averaging of points within a period that puts greater weight on more recent points (a simple moving average treats each points as equally significant). Here, V is the EMA. Note that the numbers should be subscripts.:

```python
V(1)= βV[0] + (1 — β)S[1]
V(2)= βV[1] + (1 — β)S[2]
*etc.*
```
S is the sequence. EMA's are common in market forcasting, so often S is price at time t.
β is defined from [0,1], and usually a value around 0.90 is used. The continuous update for SGD with momentum, using Andrew Ng's notation, is as follows:

```python
V[1] := β *V[0]+ (1 — β) ∇_w L(W, X, y)
W := W - αV[1]
```

α is the learning rate, as always. To be clear, ∇_w L is the gradient of the loss function with respect to the weights. \
SGD with momentum tends to work better than SGD because it gives a closer estimate of the full gradient from the batch than SGD. Additionally, the momentum helps push the update through a ravine in the right direction, where SGD tends to oscillate back and forth on in the short way. In my course's notation:

```python
v[k+1] = β v[k] + ∇_θ L(θ[k])
α[k+1] = θ[k] - α * v[k+1]
```

Here, v is the velocity term, beta is the adjustable momentum term, θ is the model parameters (i.e. the weights or biases), and α i the learning rate as always.

### **RMS Prop**

RMS prop is a another varient of SGD to improve convergence speed.

```Latex
s[k+1] =  β S[k] + (1 - β)( ∇_θ L • ∇_θ L )
```
Test: below is in latex: \
$$\nabla J \phi \theta  $$
and as an equation
```latex
$$\nabla J \phi \theta  $$
```

### **Adam Optimizer**

### **Math Review:**

The gradient acts on a scalar field (scalar valued differentiable function) and returns a vector. For example, ∇v = [dv/dx, dv/dy, dv/dz]. The gradient points in the direction of greatest increase. \
The divergence acts on a vector field and returns a scalar. It is the outward flux of a vector field around a single point. For example, ∇•v = dv/dx + dv/y + dv/dz.

# What I will do next

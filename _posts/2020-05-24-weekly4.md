---
title: "Week of May 24"
date: '2020-05-24'
categories:
  - weekly
tags:
  - Neural Net
  - Coursework
---

( *In progress...* )

# What I did
- review Einstein notation. I decided if I really want to understand neural networks, and not just know how they work, I need to drill the math into my head. There are a lot of inner products and summations, so it seemed fitting to write everything in Einstein notation.
- I've been thinking about starting a new kaggle project and moving on from NLP slightly. The jobs I'm most interested in, at the moment, are ones where implimenting recommender system is the main objective i.e. suggesting movies on netflix, songs on spotify, or art up for auction at Sotheby's.

# What I learned
- **Jaccard Validation**:
- **Stochastic Gradient Descent with Momentum**: Compared to regular SGD, this can greatly reduce time to convergence. The general idea is to multiply the gradient term with a factor that depends on 
- **
- **Adam Optimizer**:

# What I will do next

---
title: "Week of July 26th"
date: '2020-07-26'
categories:
  - weekly
tags:
  - CNN
  - NN
---

## What I did

- I worked a little on my chatbot. Preprocessing works. Only need to decide how to deal with emojis next.
- I started reviewing/studying for my deep learning course. 

## What I learned

### Review for Test

- We need activation functions for neural networks and more complicated machine learning models in order to introduce non-linearity. If you don't have an activation function, or you have a linear activation function, you end up just doing affine transformations. This means there's no point in having more complicated layers, and you might as well just have one layer with different weights and biases.

- To calculate the number of parameters in a CNN, use the formula:

$$
\big(\frac{Wâˆ’K+2P}{S}\big)+1
$$

For example, consider the output size of a CNN with an input size of 128X128X3 when 40 filters of size 5X5 are applied to it?

- W is the input volume - in this case 128
- K is the Kernel size - in this case 5
- P is the padding - in this case 0, since not stated
- S is the stride - which you have not provided.

```python
Output_Shape = (128-5+0)/1+1
Output_Shape = (124,124,40)
```

- Note, zero padding is helping for the first few layers in order to conserve spacial dimensionality (input volume = output volume).

## What I will do next
